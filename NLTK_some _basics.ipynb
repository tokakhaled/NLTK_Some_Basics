{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokanization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello , my name is toka and i want to sleep.',\n",
       " 'I want to go to univesty today']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "txt =\"hello , my name is toka and i want to sleep. I want to go to univesty today  \"\n",
    "sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " ',',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'toka',\n",
       " 'and',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'sleep',\n",
       " '.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'univesty',\n",
       " 'today']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      ",\n",
      "my\n",
      "name\n",
      "is\n",
      "toka\n",
      "and\n",
      "i\n",
      "want\n",
      "to\n",
      "sleep\n",
      ".\n",
      "I\n",
      "want\n",
      "to\n",
      "go\n",
      "to\n",
      "univesty\n",
      "today\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(txt):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop word \n",
    "is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doesn', 'my', 'have', \"should've\", 'do', 'so', 'not', 'now', \"won't\", 'because', 'only', 'with', 'then', 'are', 'same', 'am', 'very', 't', 'has', \"hadn't\", 'off', 'herself', 'd', 'at', \"doesn't\", 'themselves', 'yours', 'both', 's', \"it's\", 'where', 'you', 'aren', \"isn't\", 'any', \"that'll\", 'shan', 'your', \"needn't\", 'after', \"you'll\", 'how', 'should', \"you'd\", 'all', 'himself', 're', \"hasn't\", 'ours', 'theirs', \"mightn't\", 'having', 'or', 'does', 'own', 'between', 'his', \"couldn't\", 'its', 'don', 'she', 'no', 'too', 'again', 'they', \"you're\", 'will', 'under', 'in', 'than', \"aren't\", 'ma', 'didn', \"don't\", 'was', 'just', 'we', 'about', 'yourselves', \"wasn't\", 'down', 'more', \"shouldn't\", 'this', \"mustn't\", 'hers', 'a', 'from', 'as', 'me', 'out', 'myself', 'isn', 'itself', 'i', 'them', 'here', 'against', 'that', 'hasn', 'by', 'on', 'wouldn', 'were', 'while', 'the', \"you've\", 'those', 'who', 'wasn', 'before', 'be', 'had', 'couldn', 'once', 'him', 'needn', \"wouldn't\", 'y', 'to', 'nor', 'why', 'he', 've', 'of', 'o', 'some', 'few', 'during', 'did', 'mustn', 'mightn', 'above', 'haven', 'her', 'when', 'but', 'which', \"didn't\", 'there', 'for', 'hadn', 'whom', \"she's\", 'up', 'weren', 'being', \"shan't\", 'below', 'what', 'until', 'such', 'doing', 'and', \"weren't\", 'ain', 'into', \"haven't\", 'our', 'most', 'can', 'shouldn', 'is', 'other', 'through', 'over', 'it', 'further', 'these', 'their', 'been', 'ourselves', 'll', 'if', 'yourself', 'an', 'm', 'each', 'won'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_word_en =set(stopwords.words('english'))\n",
    "print(stop_word_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ممن', 'كيت', 'الذي', 'بيد', 'لكيلا', 'عل', 'سوى', 'هكذا', 'لست', 'لستما', 'والذين', 'لم', 'كأن', 'والذي', 'قد', 'به', 'ذينك', 'كلتا', 'بك', 'إذن', 'هنا', 'ريث', 'كيفما', 'مما', 'لهن', 'لعل', 'هما', 'منها', 'لهما', 'عند', 'أكثر', 'فيما', 'هؤلاء', 'شتان', 'إليكما', 'إذ', 'تلكما', 'اللذين', 'هناك', 'سوف', 'متى', 'إما', 'آي', 'إلا', 'لسنا', 'كليهما', 'ما', 'ذين', 'بما', 'ليستا', 'لكم', 'أولئك', 'اللاتي', 'عدا', 'كلاهما', 'لسن', 'ليس', 'لك', 'بها', 'آها', 'حين', 'حاشا', 'أنتما', 'كي', 'مذ', 'في', 'هل', 'تي', 'ذه', 'منه', 'هاته', 'هاهنا', 'لا', 'بكم', 'إنا', 'أنا', 'لئن', 'ذلك', 'أن', 'هاتي', 'ثم', 'بهم', 'حبذا', 'حيثما', 'لكما', 'بهن', 'مهما', 'كلما', 'لاسيما', 'إنه', 'فإذا', 'ماذا', 'أينما', 'ذلكن', 'ليست', 'آه', 'بكما', 'أين', 'حيث', 'ذو', 'ذوا', 'التي', 'ذي', 'كلا', 'ها', 'ولو', 'كأي', 'هلا', 'أنى', 'فيه', 'كل', 'بماذا', 'لولا', 'الذين', 'لكي', 'بخ', 'كأنما', 'وما', 'اللذان', 'ألا', 'إليكم', 'فإن', 'لها', 'وإذا', 'هذين', 'وهو', 'أم', 'على', 'أنتن', 'لدى', 'لكن', 'فلا', 'بي', 'نحن', 'ته', 'تين', 'لستم', 'كذلك', 'ذا', 'وإن', 'بين', 'أيها', 'ذلكم', 'هيت', 'ليسوا', 'فيم', 'لوما', 'عن', 'ليسا', 'بمن', 'بس', 'إليكن', 'ذواتي', 'بلى', 'إليك', 'لكنما', 'كما', 'اللتيا', 'هذي', 'غير', 'هاتين', 'هن', 'دون', 'عما', 'هنالك', 'نعم', 'لي', 'ولكن', 'أوه', 'كم', 'اللتين', 'هم', 'أف', 'هو', 'لستن', 'خلا', 'لهم', 'هاتان', 'منذ', 'هذان', 'هيا', 'إيه', 'ذان', 'كليكما', 'وإذ', 'لما', 'يا', 'فيها', 'لو', 'هي', 'إنما', 'ذانك', 'هيهات', 'ذات', 'اللتان', 'لن', 'ذاك', 'كيف', 'بعض', 'ذلكما', 'إذما', 'هذا', 'بنا', 'هاك', 'مه', 'مع', 'نحو', 'بكن', 'إلى', 'عليه', 'كأين', 'لنا', 'إي', 'ثمة', 'إن', 'بل', 'أنت', 'أنتم', 'تينك', 'أولاء', 'حتى', 'فمن', 'ولا', 'من', 'أما', 'أي', 'أقل', 'أو', 'بعد', 'تلك', 'اللائي', 'بهما', 'تلكم', 'ليت', 'هذه', 'إذا', 'ذواتا', 'عليك', 'عسى', 'كذا', 'ومن', 'له', 'اللواتي'}\n"
     ]
    }
   ],
   "source": [
    "stop_word =set(stopwords.words('arabic'))\n",
    "print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'want', 'to', 'set', 'alone', 'without', 'any', 'body', 'because', 'i', \"'m\", 'so', 'angry']\n",
      "['want', 'set', 'alone', 'without', 'body', \"'m\", 'angry']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_word =set(stopwords.words('english'))\n",
    "data=\"i want to set alone without any body because i'm so angry  \"\n",
    "words = word_tokenize(data)\n",
    "print (words)\n",
    "filterdWords=[]\n",
    "for w in words:\n",
    "    if w not in stop_word:\n",
    "        filterdWords.append(w)\n",
    "print (filterdWords)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization \n",
    "is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. \n",
    "\n",
    "Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words\n",
    "\n",
    "**After lemmatization, we will be getting a valid word that means the same thing.\n",
    "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.**\n",
    "\n",
    "\n",
    "xamples of lemmatization:\n",
    "\n",
    "-> rocks : rock\n",
    "-> corpora : corpus\n",
    "-> better : good\n",
    "\n",
    "One major difference with stemming is that lemmatize takes a part of speech parameter, “pos” If not supplied, the default is “noun.”\n",
    "\n",
    "Below is the implementation of lemmatization words using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "print ('rocks :' ,lem.lemmatize('rocks'))\n",
    "print ('corpora :' ,lem.lemmatize('corpora'))\n",
    "print ('better :' ,lem.lemmatize('better', pos =\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stone\n",
      "speak\n",
      "bedroom\n",
      "joke\n",
      "believ\n",
      "----------------------------\n",
      "stone\n",
      "speaking\n",
      "bedroom\n",
      "joke\n",
      "belief\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "print (stemmer.stem(\"stones\"))\n",
    "print (stemmer.stem(\"speaking\"))\n",
    "print (stemmer.stem(\"bedroom\"))\n",
    "print (stemmer.stem(\"jokes\"))\n",
    "print (stemmer.stem(\"believes\"))\n",
    "print('----------------------------')\n",
    "print (lemmatizer.lemmatize(\"stones\"))\n",
    "print (lemmatizer.lemmatize(\"speaking\"))\n",
    "print (lemmatizer.lemmatize(\"bedroom\"))\n",
    "print (lemmatizer.lemmatize(\"jokes\"))\n",
    "print (lemmatizer.lemmatize(\"believes\")) ##pos = noun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition \n",
    "WordNet is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing.\n",
    "\n",
    "Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. Synset instances are the groupings of synonymous words that express the the same concept. Some of the words have only one Synset and some have several."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a symptom of some physical hurt or disorder\n",
      "['the patient developed severe pain and distension']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets(\"pain\")\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syn tag :  n\n",
      "Syn tag :  v\n",
      "Syn tag :  a\n",
      "Syn tag :  r\n"
     ]
    }
   ],
   "source": [
    "syn = wordnet.synsets('hello')[0] \n",
    "print (\"Syn tag : \", syn.pos()) \n",
    "  \n",
    "syn = wordnet.synsets('doing')[0] \n",
    "print (\"Syn tag : \", syn.pos()) \n",
    "  \n",
    "syn = wordnet.synsets('beautiful')[0] \n",
    "print (\"Syn tag : \", syn.pos()) \n",
    "  \n",
    "syn = wordnet.synsets('quickly')[0] \n",
    "print (\"Syn tag : \", syn.pos()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 84 matches:\n",
      "[ Moby Dick by Herman Melville 1851 ] ETYMO\n",
      "hale must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab \n",
      "e same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye know \n",
      "all . Death and devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Mo\n",
      "en , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \" Captain Ahab \n",
      "by Dick ye have seen -- Moby Dick -- Moby Dick !\" \" Captain Ahab ,\" said Starb\n",
      "r . \" Captain Ahab , I have heard of Moby Dick -- but it was not Moby Dick tha\n",
      "heard of Moby Dick -- but it was not Moby Dick that took off thy leg ?\" \" Who \n",
      "aye , my hearties all round ; it was Moby Dick that dismasted me ; Moby Dick t\n",
      "it was Moby Dick that dismasted me ; Moby Dick that brought me to this dead st\n",
      " the white whale ; a sharp lance for Moby Dick !\" \" God bless ye ,\" he seemed \n",
      "e the white whale ? art not game for Moby Dick ?\" \" I am game for his crooked \n",
      "athful whaleboat ' s bow -- Death to Moby Dick ! God hunt us all , if we do no\n",
      " God hunt us all , if we do not hunt Moby Dick to his death !\" The long , barb\n",
      " no bowels to feel fear ! CHAPTER 41 Moby Dick . I , Ishmael , was one of that\n",
      "l individualizing tidings concerning Moby Dick . It was hardly to be doubted ,\n",
      "uestion must have been no other than Moby Dick . Yet as of late the Sperm Whal\n",
      "y accident ignorantly gave battle to Moby Dick ; such hunters , perhaps , for \n",
      "lating and piling their terrors upon Moby Dick ; those things had gone far to \n",
      "agencies , which eventually invested Moby Dick with new terrors unborrowed fro\n",
      "fishermen recalled , in reference to Moby Dick , the earlier days of the Sperm\n",
      "e things were ready to give chase to Moby Dick ; and a still greater number wh\n",
      "ned , was the unearthly conceit that Moby Dick was ubiquitous ; that he had ac\n",
      "r in their superstitions ; declaring Moby Dick not only ubiquitous , but immor\n",
      "kle - shaped lower jaw beneath him , Moby Dick had reaped away Ahab ' s leg , \n"
     ]
    }
   ],
   "source": [
    "import nltk.corpus  \n",
    "from nltk.text import Text  \n",
    "moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.concordance(\"moby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 'PRP'),\n",
       " ('dreamed', 'VBD'),\n",
       " ('each', 'DT'),\n",
       " ('man', 'NN'),\n",
       " ('according', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('their', 'PRP$'),\n",
       " ('generatio', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('firstborn', 'NN'),\n",
       " ('said', 'VBD'),\n",
       " ('unto', 'JJ'),\n",
       " ('Laban', 'NNP')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "word = word_tokenize(\"we dreamed each man according to their generatio the firstborn said unto Laban\")\n",
    "pos_tag(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man\n",
      "generatio\n",
      "firstborn\n"
     ]
    }
   ],
   "source": [
    "pos=pos_tag(word)\n",
    "for pair in pos:\n",
    "    for a in pair:\n",
    "        if(a == 'NN'):\n",
    "            print(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
